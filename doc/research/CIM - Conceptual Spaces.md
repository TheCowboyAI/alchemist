Conceptual spaces, memory engrams, and LLM tokens share fundamental similarities in how they represent and process information, despite originating from different fields.

## Representational Structures

Conceptual spaces, as proposed by Peter GÃ¤rdenfors, are geometric structures representing quality dimensions where points denote objects and regions denote concepts[2]. This geometric approach allows for representing semantic relationships spatially, with natural categories forming convex regions[2]. Similarly, memory engrams are physical or chemical changes in the brain that store memories as connected neural ensembles[3]. LLM tokens function as basic units that, when embedded in vector spaces, create semantic relationships between concepts[4][5].

## Information Encoding

All three systems encode information in ways that capture semantic relationships:

- **Conceptual spaces** organize information geometrically, with conceptual similarity represented by proximity in the space[1][2]
- **Memory engrams** store information through persistent physical/chemical changes in neural networks, with connected engram cell ensembles forming an "engram complex"[3]
- **LLM tokens** are transformed into embeddings (multi-valued numeric vectors) that represent semantic relationships between words and concepts[4][5]

## Semantic Networks

Each system creates networks of meaning:

- In conceptual spaces, concepts are connected through similarity relationships in a geometric structure[1][2]
- Memory engrams form networks where associations between concepts are encoded by cells shared between neuronal assemblies ("overlap" of concepts)[7]
- LLMs create conceptual spaces through word embeddings that exhibit small-world properties with high clustering coefficients and short path lengths[5]

## Retrieval Mechanisms

The retrieval process shows notable parallels:

- Conceptual spaces allow for retrieval based on similarity to prototypes within convex regions[2]
- Memory engrams are reactivated by stimuli present during learning, with retrieval depending on both the engram and the retrieval cue[6]
- LLMs retrieve information through vector similarity in embedding space, with larger models generating more intricate spaces with richer relational structures[5][9]

## Evolution Beyond Tokens

Recent research suggests a shift from token-based to concept-based AI models. Meta's Large Concept Models (LCMs) work with concepts rather than tokens, similar to how conceptual spaces operate at higher levels of abstraction[8][11]. This approach more closely resembles human cognition, which operates at multiple levels of abstraction beyond individual words[8].

The convergence of these three frameworks suggests a common underlying principle: information is most effectively stored, processed, and retrieved when organized in semantic networks that capture relationships between concepts rather than isolated units of information.

Citations:
[1] https://direct.mit.edu/books/monograph/4012/The-Geometry-of-MeaningSemantics-Based-on
[2] https://en.wikipedia.org/wiki/Conceptual_space
[3] https://pmc.ncbi.nlm.nih.gov/articles/PMC7577560/
[4] https://learn.microsoft.com/en-us/dotnet/ai/conceptual/understanding-tokens
[5] https://arxiv.org/abs/2502.11380
[6] https://arxiv.org/html/2402.16505v2
[7] https://pmc.ncbi.nlm.nih.gov/articles/PMC8754331/
[8] https://aipapersacademy.com/large-concept-models/
[9] https://www.linkedin.com/pulse/understanding-core-components-llms-vectors-tokens-embeddings-jain-dlv6e
[10] https://www.nature.com/articles/s41380-023-02137-5
[11] https://www.linkedin.com/pulse/next-evolution-ai-trading-tokens-concepts-large-concept-ganesh-raju-cdgwc
[12] https://www.penguinrandomhouse.ca/books/658771/conceptual-spaces-by-peter-gardenfors/9780262572194
[13] https://picower.mit.edu/news/engrams-emerging-basic-unit-memory
[14] https://www.youtube.com/watch?v=U6cg0KIwozg
[15] https://www.evl.uic.edu/sugimoto/memRem.html
[16] https://drlee.io/what-is-the-role-of-tokens-in-language-models-do-more-tokens-make-the-llm-model-more-or-less-f35dff4b1e0f
[17] https://systenics.ai/blog/2024-09-30-understanding-tokenizers-in-large-language-models/
[18] https://blog.mlq.ai/tokens-context-window-llms/
[19] https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
[20] https://www.koyeb.com/blog/what-are-large-language-models
[21] https://www.reddit.com/r/LocalLLaMA/comments/160uuzy/what_does_llms_token_context_actually_mean/
[22] https://arxiv.org/html/2402.13449v1
[23] https://www.frontiersin.org/journals/molecular-neuroscience/articles/10.3389/fnmol.2024.1342622/full
[24] https://seantrott.substack.com/p/tokenization-in-large-language-models
[25] https://pubmed.ncbi.nlm.nih.gov/39689709/
[26] https://arxiv.org/html/2502.11380v1
[27] https://www.pinecone.io/blog/memory-for-open-source-llms/
[28] https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1009691
[29] https://www.themoonlight.io/es/review/exploring-the-small-world-of-word-embeddings-a-comparative-study-on-conceptual-spaces-from-llms-of-different-scales
[30] https://www.flow-ai.com/blog/advancing-long-context-llm-performance-in-2025
[31] https://www.reddit.com/r/neuro/comments/u1nit2/brainwide_mapping_reveals_that_engrams_for_a/
[32] https://lcnwww.epfl.ch/gerstner/PUBLICATIONS/Gastaldi-Gerstner2024.pdf
[33] https://www.youtube.com/watch?v=87O9fnu8BTU
[34] https://direct.mit.edu/books/monograph/2532/Conceptual-SpacesThe-Geometry-of-Thought
[35] https://sites.cc.gatech.edu/classes/AY2013/cs7601_spring/papers/gaerdenfors.pdf
[36] https://www.goodreads.com/book/show/1877443.Conceptual_Spaces
[37] https://cs.nyu.edu/faculty/davise/commonsense01/final/Gardenfors.pdf
[38] https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1554/1453
[39] https://www.nature.com/articles/s41467-022-29384-4
[40] https://pmc.ncbi.nlm.nih.gov/articles/PMC9065729/
[41] https://en.wikipedia.org/wiki/Engram_(neuropsychology)
[42] https://pubmed.ncbi.nlm.nih.gov/31896692/
[43] https://www.johno.com/tokens-vs-words
[44] https://kelvin.legal/understanding-large-language-models-words-versus-tokens
[45] https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e
[46] https://airbyte.com/data-engineering-resources/llm-tokenization
[47] https://blog.devgenius.io/understanding-tokens-and-tokenization-in-large-language-models-1058cd24b944
[48] https://www.reddit.com/r/explainlikeimfive/comments/1c6wuc9/eli5_how_does_the_token_system_llms_use_work/
[49] https://www.biorxiv.org/content/10.1101/2021.03.12.434964v1.full-text
